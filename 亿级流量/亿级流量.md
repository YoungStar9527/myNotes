

# 1 zookeeper安装(集群)

**1 将提供的zookeeper-3.4.5.tar.gz拷贝到/usr/local目录下。**

**2 对zookeeper-3.4.5.tar.gz进行解压缩：**

**3 对zookeeper目录进行重命名：**

```shell
mv zookeeper-3.4.5 zk
```

**4 配置zookeeper相关的环境变量**

```
vim  /etc/profile
```

**PS:zookeeper安装的前提是已经安装好了JVM**

```shell
#profile加入zk相关环境变量，并设置到path中
export ZOOKEEPER_HOME=/usr/local/zk
export PATH=$PATH:$ZOOKEEPER_HOME/bin

```

**5 设置配置文件**

```shell
cd zk/conf
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg
```

修改配置文件

```shell
#zoo.cfg中修改对应data目录位置
dataDir=/usr/local/zk/data
#新增对应集群配置，放在最后面即可
server.0=eshop-cache01:2888:3888	
server.1=eshop-cache02:2888:3888
server.2=eshop-cache03:2888:3888
```

**6 建立输出目录**

```shell
cd zk
#在zk的根目录下建立data，就是对应zoo.cfg中的dataDir的位置
mkdir data
```

**7 在对应输出目录建立myid文件**

```shell
cd data
#设置myid文件的值为0即可
vi myid
0
```

**8 集群的配置**

配置3节点集群

在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到机器2和机器3上即可。唯一的区别是标识号分别设置为1和2。

**9 启动并检查是否安装成功**

分别在三台机器上执行：

```shell
#启动
zkServer.sh start
#检查ZooKeeper状态：，应该是一个leader，两个follower
zkServer.sh status
```


jps：检查三个节点是否都有QuromPeerMain进程

![image-20210904172804844](亿级流量.assets/image-20210904172804844.png)

**扩展-scp使用：**

scp是用来将本服务器上的文件复制到其他服务器上的命令

```shell
#scp 目录/文件(目录就前加-r) 账户@ip地址:对应目录
#回车后需要验证对应服务器密码
scp -r zk root@192.168.31.102:/usr/local
scp  zk.taz root@192.168.31.102:/usr/local
```

# 2 kafka安装(集群)

## 2.1 前言简介

scala，很多比如大数据领域里面的spark（计算引擎）就是用scala编写的，kafka也是基于scala开发的

Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。

Scala 运行在 Java 虚拟机上，并兼容现有的 Java 程序。

Scala 源代码被编译成 Java 字节码，所以它可以运行于 JVM 之上，并可以调用现有的 Java 类库。

## 2.2 scala安装

1 将课程提供的scala-2.11.4.tgz拷贝到/usr/local目录下。

2 对scala-2.11.4.tgz进行解压缩

3 对scala目录进行重命名

```shell
mv scala-2.11.4 scala
```

4 配置scala相关环境变量

```shell
vim /etc/profile
```

环境变量

```shell
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
```

5 查看scala是否安装成功：

```shell
scala -version
```

![image-20210905143338295](亿级流量.assets/image-20210905143338295.png)

6 按照上述步骤在其他机器上都安装好scala

## 2.3 kafka安装(集群)

1 将课程提供的kafka_2.9.2-0.8.1.tgz拷贝到/usr/local目录下

2 对kafka_2.9.2-0.8.1.tgz进行解压缩：

3 对kafka目录进行改名：

```shell
mv kafka_2.9.2-0.8.1 kafka
```

4 配置kafka

```
vim /usr/local/kafka/config/server.properties
```

配置文件

```shell
#依次增长的整数，0、1、2，集群中Broker的唯一id
broker.id=0
#集群配置
zookeeper.connect=192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181
```

5 安装slf4j

将课程提供的slf4j-1.7.6.zip解压
找到slf4j-nop-1.7.6.jar包
复制到kafka的libs目录下面

![image-20210905144508188](亿级流量.assets/image-20210905144508188.png)

6 解决kafka Unrecognized VM option 'UseCompressedOops'问题

```shell
#修改脚本文件
vim /usr/local/kafka/bin/kafka-run-class.sh 
```

脚本文件

```shell
if [ -z "$KAFKA_JVM_PERFORMANCE_OPTS" ]; then
  KAFKA_JVM_PERFORMANCE_OPTS="-server  -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true"
fi
#去掉-XX:+UseCompressedOops即可
```

去掉后

![image-20210905144643363](亿级流量.assets/image-20210905144643363.png)

7 集群配置

按照上述步骤在另外两台机器分别安装kafka。用scp把kafka拷贝到其他机器即可。
唯一区别的，就是server.properties中的broker.id，要设置为1和2

8 启动kafka集群

在三台机器上的kafka目录下，分别执行以下命令：

```shell
#启动
nohup bin/kafka-server-start.sh config/server.properties &
```

使用jps检查启动是否成功

![image-20210905144810149](亿级流量.assets/image-20210905144810149.png)

9 使用基本命令检查kafka是否搭建成功

```shell
#创建topic
bin/kafka-topics.sh --zookeeper 192.168.31.101:2181,192.168.31.102:2181,192.168.31.103:2181 --topic test --replication-factor 1 --partitions 1 --create

#创建生产者,这里会进入Kafka，输入相关信息，订阅对应topic的消费就能消费到
bin/kafka-console-producer.sh --broker-list 192.168.31.101:9092,192.168.31.102:9092,192.168.31.103:9092 --topic test
#创建消费者，这里也会进入kafka，就能展示对应topic生产的信息
bin/kafka-console-consumer.sh --zookeeper 192.168.31.101:2181,192.168.31.102:2181,192.168.31.103:2181 --topic test --from-beginning
```

生产

![image-20210905145439193](亿级流量.assets/image-20210905145439193.png)

消费

![image-20210905145522306](亿级流量.assets/image-20210905145522306.png)

# 3 亿级流量redis

参考笔记部分的redis内容

## 3.1 redis的LRU缓存清除算法

### 3.1.1、LRU算法概述

redis默认情况下就是使用LRU策略的，因为内存是有限的，但是如果你不断地往redis里面写入数据，那肯定是没法存放下所有的数据在内存的

所以**redis默认情况下，当内存中写入的数据很满之后，就会使用LRU算法清理掉部分内存中的数据，腾出一些空间来，然后让新的数据写入redis缓存中**

**LRU：Least Recently Used，最近最少使用算法**

### 3.1.2、缓存清理设置

**redis.conf**

**maxmemory**，设置redis用来存放数据的最大的内存大小，一旦超出这个内存大小之后，就会立即使用LRU算法清理掉部分数据

如果用LRU，那么就是将最近最少使用的数据从缓存中清除出去

对于64 bit的机器，如果maxmemory设置为0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止; 但是对于32 bit的机器，有一个隐式的闲置就是3GB

**maxmemory-policy**，可以设置内存达到最大闲置后，采取什么策略来处理

（1）noeviction: 如果内存使用达到了maxmemory，client还要继续写入数据，那么就直接报错给客户端
（2）allkeys-lru: 就是我们常说的LRU算法，移除掉最近最少使用的那些keys对应的数据
（3）volatile-lru: 也是采取LRU算法，但是仅仅针对那些设置了指定存活时间（TTL）的key才会清理掉
（4）allkeys-random: 随机选择一些key来删除掉
（5）volatile-random: 随机选择一些设置了TTL的key来删除掉
（6）volatile-ttl: 移除掉部分keys，选择那些TTL时间比较短的keys

### 3.1.3、缓存清理的流程

（1）客户端执行数据写入操作
（2）redis server接收到写入操作之后，检查maxmemory的限制，如果超过了限制，那么就根据对应的policy清理掉部分数据
（3）写入操作完成执行

### 3.1.4、redis的LRU近似算法

科普一个相对来说稍微高级一丢丢的知识点

**redis采取的是LRU近似算法，也就是对keys进行采样，然后在采样结果中进行数据清理**

redis 3.0开始，在LRU近似算法中引入了pool机制，表现可以跟真正的LRU算法相当，但是还是有所差距的，不过这样可以减少内存的消耗

**redis LRU算法，是采样之后再做LRU清理的，跟真正的、传统、全量的LRU算法是不太一样的**

**maxmemory-samples**，比如5，可以设置采样的大小，如果设置为10，那么效果会更好，不过也会耗费更多的CPU资源

### 3.1.5 LRU算法实现(java)

**待补充**

### 3.1.6 扩展

在redis里面，写入key-value对的时候，是可以设置TTL，存活时间，比如你设置了60s。那么一个key-value对，在60s之后就会自动被删除

redis的使用，各种数据结构，list，set，等等

allkeys-lru

这边拓展一下思路，对技术的研究，一旦将一些技术研究的比较透彻之后，就喜欢横向对比底层的一些原理

storm，科普一下

玩儿大数据的人搞得，领域，实时计算领域，storm

storm有很多的流分组的一些策略，按shuffle分组，global全局分组，direct直接分组，fields按字段值hash后分组

分组策略也很多，但是，真正公司里99%的场景下，使用的也就是shuffle和fields，两种策略

redis，给了这么多种乱七八糟的缓存清理的算法，其实真正常用的可能也就那么一两种，allkeys-lru是最常用的

# 4 ngnix+lua缓存架构

​	**用nginx+lua去开发，所以会选择用最流行的开源方案，就是用OpenResty**

​	**nginx+lua打包在一起，而且提供了包括redis客户端，mysql客户端，http客户端在内的大量的组件**

## 4.1 OpenResty安装

1 创建目录

```shell
mkdir -p /usr/servers  
cd /usr/servers/
```

2 安装必要依赖

```shell
yum install -y readline-devel pcre-devel openssl-devel gcc
```

3 下载包并解压

```shell
wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  
tar -xzvf ngx_openresty-1.7.7.2.tar.gz  
```

4 编译

```shell
#先进入对应目录
cd /usr/servers/ngx_openresty-1.7.7.2/
#先进入对应目录
cd bundle/LuaJIT-2.1-20150120/  
#编译
make clean && make && make install  
#连接
ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit
```

5 相关配置包

下载

```shell
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
#下载
wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  
#解压
tar -xvf 2.3.tar.gz  
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
#下载
wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  
#解压
tar -xvf v0.3.0.tar.gz  
```

编译

```shell
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2  
#编译
./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  
#编译
make && make install 
```

6 查看目录，启动nginx

![image-20210905202521243](亿级流量.assets/image-20210905202521243.png)

启动nginx:

```shell
 /usr/servers/nginx/sbin/nginx
```

**PS:若对应机器安装了gitlab会有nginx冲突，需要卸载gitlab，或者临时kill进程，或者改默认端口等操作，才能启动**

## 4.2 demo级应用

### 4.2.1 nginx+lua开发的demo

1 配置nginx.conf

```shell
vim /usr/servers/nginx/conf/nginx.conf
```

在http部分添加：

```shell
lua_package_path "/usr/servers/lualib/?.lua;;";  
lua_package_cpath "/usr/servers/lualib/?.so;;";  
include lua.conf;
```

2 创建lua.conf

/usr/servers/nginx/conf下，创建一个lua.conf

```shell
server {  
    listen       80;  
    server_name  _;  
    location /lua {  
    	default_type 'text/html';  
   		content_by_lua 'ngx.say("hello world")';  
	} 
}  
```

3 验证配置是否正确：

```shell
/usr/servers/nginx/sbin/nginx -t
```

4 重新nginx加载配置

```shell
/usr/servers/nginx/sbin/nginx -s reload  
```

访问http: http://192.168.31.187/lua

5 可选-独立出lua相关代码

```shell
#创建并打开
vim /usr/servers/nginx/conf/lua/hello.lua
#加入
ngx.say("hello world"); 
```

修改lua.conf

```shell
#相关代码改为引入文件
location /lua {  
    default_type 'text/html';  
    content_by_lua_file conf/lua/hello.lua; 
}
```

7 可选-查看异常日志

```
tail -f /usr/servers/nginx/logs/error.log
cat /usr/servers/nginx/logs/error.log
```



### 4.2.2 工程化的nginx+lua项目结构

项目工程结构

```
#这是目录层级
hello
    hello.conf     
    lua              
      hello.lua
    lualib            
      *.lua
      *.so
```

1 创建目录

```
mkdir /usr/hello
```

2 创建hello.conf文件

```shell
#打开并创建
vim /usr/hello/hello.conf
#加入
server {  
    listen       80;  
    server_name  _;  
  
    location /hello {  
        default_type 'text/html';  
        #lua_code_cache off;  
        content_by_lua_file /usr/hello/lua/hello.lua;  
    }  
} 

```

4 创建lua目录及对应代码文件

```shell
mkdir /usr/hello/lua
vim /usr/hello/lua/hello.lua
#加入
ngx.say("hello world 78910");
```

5 将对应包加入到hello工程目录中

```shell
cd /usr/hello/
#将之前装好的包移动到工程中
cd -r /usr/servers/lualib/ .
```

6 修改nginx配置文件，改为引用hello工程的相关配置

```shell
vim /usr/servers/nginx/conf/nginx.conf
#对应http中的配置改为
    lua_package_path "/usr/hello/lualib/?.lua;;";
    lua_package_cpath "/usr/hello/lualib/?.so;;";
    include /usr/hello/hello.conf;
```



![image-20210905204346847](亿级流量.assets/image-20210905204346847.png)

## 4.3 基于“分发层+应用层”双层nginx架构提升缓存命中率方案分析

**1、缓存命中率低**

**如果一般来说，你默认会部署多个nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的**

![缓存命中率低的原因](亿级流量.assets/缓存命中率低的原因.png)

**2、如何提升缓存命中率**

**分发层+应用层，双层nginx**

**分发层nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模**

**将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证说只会从redis中获取一次缓存数据，后面全都是走nginx本地缓存了**

后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器

看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能

![分发层+应用层双层nginx架构](亿级流量.assets/分发层+应用层双层nginx架构.png)



## 4.4 分发层nginx以及基于lua完成基于商品id的定向流量分发策略

### 4.4.1 实现逻辑

基于OpenResty在另外两台机器上都部署一下nginx+lua的开发环境

用101和102作为应用层nginx服务器，用103作为分发层nginx

在103，也就是分发层nginx中，编写lua脚本，完成基于商品id的流量分发策略

当然了，我们这里主要会简化策略，简化业务逻辑，实际上在你的公司中，你可以随意**根据自己的业务逻辑和场景，去制定自己的流量分发策略(提高缓存命中率)**

**1、获取请求参数，比如productId**

**2、对productId进行hash**

**3、hash值对应用服务器数量取模，获取到一个应用服务器**

**4、利用http发送请求到应用层nginx**

**5、获取响应后返回**

这个就是基于商品id的定向流量分发的策略，lua脚本来编写和实现

我们作为一个流量分发的nginx，会发送http请求到后端的应用nginx上面去，所以要先引入lua http lib包

### 4.4.2 具体实现

在对应目录加入lua脚本(相当于jar包依赖)

下载解压缩，复制到目录

https://www.zixuephp.net/uploads/file/20181017/1539739400144830.rar

```shell
cd /usr/hello/lualib/resty/  
```

编辑lua脚本

```shell
vim /usr/hello/lua/hello.lua
```

编辑

```lua
local uri_args = ngx.req.get_uri_args()
local productId = uri_args["productId"]

local hosts = {"192.168.31.101", "192.168.31.102"}
local hash = ngx.crc32_long(productId)
local index = (hash % 2) + 1
backend = "http://"..hosts[index]

local requestPath = uri_args["requestPath"]
requestPath = "/"..requestPath.."?productId="..productId

local http = require("resty.http")
local httpc = http.new()

local resp, err = httpc:request_uri(backend,{
  method = "GET",
  path = requestPath,
  keepalive = false
})

if not resp then
  ngx.say("request error: ", err)
  return
end

ngx.say(resp.body)

httpc:close() 
```

发送请求测试：

http://192.168.31.103/hello?requestPath=hello&productId=1

![image-20210906214634125](亿级流量.assets/image-20210906214634125.png)

**我们就可以看到，如果你请求的是固定的某一个商品，那么就一定会将流量打到固定的一个应用nginx上面去(提高缓存命中率)**

# 5 storm

## 5.1 概述

### 5.1.1 缓存架构和storm的关系

​	因为有些热点数据相关的一些**实时处理的一些方案，比如快速预热，热点数据的实时感知和快速降级，全部要用到storm**

​	因为我们可能需要**实时的去计算出热点缓存数据，实时计算，亿级流量，高并发**，大量的请求过来

​	这个时候，你要做一些实时的计算，那么必须涉及到**分布式的一些技术，分布式的技术**，才能处理高并发，大量的请求

​	目前在时候**计算的领域，最成熟的大数据的技术，就是storm**

​	storm分布式的大数据实时计算的技术/系统

### 5.1.2 大数据/数据处理相关技术概述

**Storm：**

​	**实时缓存热点数据统计->缓存预热->缓存热点数据自动降级**

**Hive：**

​	**Hadoop生态栈里面，做数据仓库的一个系统，高并发访问下，海量请求日志的批量统计分析，日报周报月报，接口调用情况，业务使用情况，等等**

​	在一些大公司里面，是有些人是将海量的请求日志打到hive里面，做离线的分析，然后反过来去优化自己的系统

**Spark：**

​	**离线批量数据处理，比如从DB中一次性批量处理几亿数据，清洗和处理后写入Redis中供后续的系统使用**，大型互联网公司的用户相关数据

**ZooKeeper：**

​	**分布式系统的协调，分布式锁，分布式选举->高可用HA架构，轻量级元数据存储**

​	用java开发了分布式的系统架构，你的整套系统拆分成了多个部分，每个部分都会负责一些功能，互相之间需要交互和协调

​	服务A说，我在处理某件事情的时候，服务B你就别处理了

​	服务A说，我一旦发生了某些状况，希望服务B你立即感知到，然后做出相应的对策

**HBase：**

​	**海量数据的在线存储和简单查询，替代MySQL分库分表，提供更好的伸缩性**

​	java底层，对应的是海量数据，然后要做一些简单的存储和查询，同时数据增多的时候要快速扩容

​	mysql分库分表就不太合适了，mysql分库分表扩容，还是比较麻烦的

**Elasticsearch：**

​	**海量数据的复杂检索以及搜索引擎的构建，支撑有大量数据的各种企业信息化系统的搜索引擎，电商/新闻等网站的搜索引擎，等等**

​	mysql的like "%xxxx%"，更加合适一些，性能更加好

### 5.1.3 Storm解决的问题及特点

**1、mysql，hadoop与storm**

mysql：事务性系统，面临海量数据的尴尬

hadoop：离线批处理

**storm：实时计算**

mysql、hadoop与storm的关系图：

![mysql、hadoop与storm的关系](亿级流量.assets/mysql、hadoop与storm的关系-16310167350741.png)

**2、storm的特点是什么**

（1）**支撑各种实时类的项目场景：实时处理消息以及更新数据库**，基于最基础的实时计算语义和API（实时数据处理领域）；对实时的数据流持续的进行查询或计算，同时将最新的计算结果持续的推送给客户端展示，同样基于最基础的实时计算语义和API（实时数据分析领域）；对耗时的查询进行并行化，基于DRPC，即分布式RPC调用，单表30天数据，并行化，每个进程查询一天数据，最后组装结果

storm做各种实时类的项目都ok

（2）**高度的可伸缩性：如果要扩容，直接加机器**，调整storm计算作业的并行度就可以了，storm会自动部署更多的进程和线程到其他的机器上去，无缝快速扩容

扩容起来，超方便

（3）**数据不丢失的保证：storm的消息可靠机制开启后，可以保证一条数据都不丢**

数据不丢失，也不重复计算

（4）**超强的健壮性：从历史经验来看，storm比hadoop、spark等大数据类系统，健壮的多的多，因为元数据全部放zookeeper，不在内存中，随便挂都不要紧**

特别的健壮，稳定性和可用性很高

（5）**使用的便捷性：核心语义非常的简单，开发起来效率很高**

用起来很简单，开发API还是很简单的

**PS:海量高并发大数据，高并发的请求数据，分布式的系统，流式处理的分布式系统**

**3、strom相关的技术**

JStorm，阿里。Storm，clojure编程预压，Java重新写了一遍，Galaxy流式计算的系统

## 5.2 strom的核心概念

**1、Storm的集群架构**

Nimbus->Supervisor->Worker->Executor->Task

ZooKeeper

storm集群架构图:

![storm集群架构](亿级流量.assets/storm集群架构.png)

**2、Storm的核心概念**

Topology，Spout，Bolt，Tuple，Stream

**拓扑(Topology)：务虚的一个概念(对应多个worker)**

**Spout：数据源的一个代码组件**，就是我们可以实现一个spout接口，写一个java类，**在这个spout代码中，我们可以自己去数据源获取数据**，比如说从kafka中消费数据**(对应多个executor,task)**

**bolt：一个业务处理的代码组件**，spout会将数据传送给bolt，各种bolt还可以串联成一个计算链条，java类实现了一个bolt接口

一堆spout+bolt，就会组成一个topology，就是一个拓扑，实时计算作业，spout+bolt，一个拓扑涵盖数据源获取/生产+数据处理的所有的代码逻辑，topology

**tuple：就是一条数据，每条数据都会被封装在tuple中，在多个spout和bolt之间传递**

**stream：就是一个流**，务虚的一个概念，抽象的概念，**源源不断过来的tuple，就组成了一条数据流**

**storm核心概念图(下图对应：1个Topology->4个worker->6个executor->n个task)：**

![storm核心概念](亿级流量.assets/storm核心概念.png)

## 5.3 并行度和流分组

**并行度：Worker->Executor->Task**

**流分组：**

​	**Task与Task之间的数据流向关系**

​	**Shuffle Grouping：随机发射，负载均衡**

​	**Fields Grouping：根据某一个，或者某些个，fields(字段)，进行分组**，那一个或者多个fields如果值完全相同的话，那么这些tuple，就会发送给下游bolt的其中固定的一个task

你发射的每条数据是一个tuple，每个tuple中有多个field作为字段

比如tuple，3个字段，name，age，salary

{"name": "tom", "age": 25, "salary": 10000} -> tuple -> 3个field，name，age，salary

All Grouping
Global Grouping
None Grouping
Direct Grouping
Local or Shuffle Grouping

**并行度和流分组图:**

![并行度和流分组](亿级流量.assets/并行度和流分组.png)

## 5.4 strom的demo程序

​	**大数据，入门程序，wordcount，单词计数**

​	**可以认为，storm源源不断的接收到一些句子，然后你需要实时的统计出句子中每个单词的出现次数**

 * 单词计数拓扑
 * storm，最基本的开发，就够了，**java开发广告计费系统，大量的流量的引入和接入，就是用storm做得**
 * **用storm，主要是用它的成熟的稳定的易于扩容的分布式系统的特性**
 * java工程师，来说，做一些简单的storm开发，掌握到这个程度差不多就够了

pom

```xml
  	<dependencies>
    	<dependency>
      		<groupId>junit</groupId>
      		<artifactId>junit</artifactId>
      		<version>4.6</version>
      		<scope>test</scope>
    	</dependency>
    	<dependency>
      		<groupId>org.apache.storm</groupId>
      		<artifactId>storm-core</artifactId>
      		<version>1.1.0</version>
    	</dependency>
    	<dependency>
      		<groupId>commons-collections</groupId>
      		<artifactId>commons-collections</artifactId>
      		<version>3.2.1</version>
    	</dependency>
  	</dependencies>
```

spout提供数据，正常来说是要从kafka或者mysql等外部获取数据，这里demo利用随机数，生成单词数据

```java
	/**
	 * spout
	 * 
	 * spout，继承一个基类，实现接口，这个里面主要是负责从数据源获取数据
	 * 
	 * 我们这里作为一个简化，就不从外部的数据源去获取数据了，只是自己内部不断发射一些句子
	 * 
	 * @author Administrator
	 *
	 */
	public static class RandomSentenceSpout extends BaseRichSpout {

		private static final long serialVersionUID = 3699352201538354417L;
		
		private static final Logger LOGGER = LoggerFactory.getLogger(RandomSentenceSpout.class);

		private SpoutOutputCollector collector;
		private Random random;
		
		/**
		 * open方法
		 * 
		 * open方法，是对spout进行初始化的
		 * 
		 * 比如说，创建一个线程池，或者创建一个数据库连接池，或者构造一个httpclient
		 * 
		 */
		@SuppressWarnings("rawtypes")
		public void open(Map conf, TopologyContext context,
				SpoutOutputCollector collector) {
			// 在open方法初始化的时候，会传入进来一个东西，叫做SpoutOutputCollector
			// 这个SpoutOutputCollector就是用来发射数据出去的
			this.collector = collector;
			// 构造一个随机数生产对象
			this.random = new Random();
		}
		
		/**
		 * nextTuple方法
		 * 
		 * 这个spout类，之前说过，最终会运行在task中，某个worker进程的某个executor线程内部的某个task中
		 * 那个task会负责去不断的无限循环调用nextTuple()方法
		 * 只要的话呢，无限循环调用，可以不断发射最新的数据出去，形成一个数据流
		 * 
		 */
		public void nextTuple() {
			Utils.sleep(100); 
			String[] sentences = new String[]{"the cow jumped over the moon", "an apple a day keeps the doctor away",
					"four score and seven years ago", "snow white and the seven dwarfs", "i am at two with nature"};
			String sentence = sentences[random.nextInt(sentences.length)];
			LOGGER.info("【发射句子】sentence=" + sentence);  
			// 这个values，你可以认为就是构建一个tuple
			// tuple是最小的数据单位，无限个tuple组成的流就是一个stream
			collector.emit(new Values(sentence)); 
		}

		/**
		 * declareOutputFielfs这个方法
		 * 
		 * 很重要，这个方法是定义一个你发射出去的每个tuple中的每个field的名称是什么
		 * 
		 */
		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields("sentence"));   
		}
		
	}
```

第一个bolt逻辑为接收spout发射的句子，然后把句子分为一个一个的单词，再去发射

```java
	/**
	 * 写一个bolt，直接继承一个BaseRichBolt基类
	 * 
	 * 实现里面的所有的方法即可，每个bolt代码，同样是发送到worker某个executor的task里面去运行
	 * 
	 * @author Administrator
	 *
	 */
	public static class SplitSentence extends BaseRichBolt {
		
		private static final long serialVersionUID = 6604009953652729483L;
		
		private OutputCollector collector;
		
		/**
		 * 对于bolt来说，第一个方法，就是prepare方法
		 * 
		 * OutputCollector，这个也是Bolt的这个tuple的发射器
		 * 
		 */
		@SuppressWarnings("rawtypes")
		public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
			this.collector = collector;
		}
		
		/**
		 * execute方法
		 * 
		 * 就是说，每次接收到一条数据后，就会交给这个executor方法来执行
		 * 
		 */
		public void execute(Tuple tuple) {
			String sentence = tuple.getStringByField("sentence"); 
			String[] words = sentence.split(" "); 
			for(String word : words) {
				collector.emit(new Values(word)); 
			}
		}

		/**
		 * 定义发射出去的tuple，每个field的名称
		 */
		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields("word"));   
		}
		
	}
```

第二个bolt为接受单词，然后根据单词出现的次数来计数

```java
	public static class WordCount extends BaseRichBolt {

		private static final long serialVersionUID = 7208077706057284643L;
		
		private static final Logger LOGGER = LoggerFactory.getLogger(WordCount.class);

		private OutputCollector collector;
		private Map<String, Long> wordCounts = new HashMap<String, Long>();
		
		@SuppressWarnings("rawtypes")
		public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
			this.collector = collector;
		}
		
		public void execute(Tuple tuple) {
			String word = tuple.getStringByField("word");
			
			Long count = wordCounts.get(word);
			if(count == null) {
				count = 0L;
			}
			count++;
			
			wordCounts.put(word, count);
			
			LOGGER.info("【单词计数】" + word + "出现的次数是" + count);  
			
			collector.emit(new Values(word, count));
		}

		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields("word", "count"));    
		}
		
	}
```

测试单词技术demo

```java
	public static void main(String[] args) {
		// 在main方法中，会去将spout和bolts组合起来，构建成一个拓扑
		TopologyBuilder builder = new TopologyBuilder();
	
		// 这里的第一个参数的意思，就是给这个spout设置一个名字
		// 第二个参数的意思，就是创建一个spout的对象
		// 第三个参数的意思，就是设置spout的executor有几个
		builder.setSpout("RandomSentence", new RandomSentenceSpout(), 2);
		builder.setBolt("SplitSentence", new SplitSentence(), 5)
				.setNumTasks(10)
				.shuffleGrouping("RandomSentence");
        //fieldsGrouping按字段进行分组
		// 这个很重要，就是说，相同的单词，从SplitSentence发射出来时，一定会进入到下游的指定的同一个task中
		// 只有这样子，才能准确的统计出每个单词的数量
		// 比如你有个单词，hello，下游task1接收到3个hello，task2接收到2个hello
		// 5个hello，全都进入一个task
		builder.setBolt("WordCount", new WordCount(), 10)
				.setNumTasks(20)
            //fieldsGrouping按字段进行分组,值相同的元组，一定让同一个bolt线程来处理
				.fieldsGrouping("SplitSentence", new Fields("word"));  
		
		Config config = new Config();
	
		// 说明是在命令行执行，打算提交到storm集群上去
		if(args != null && args.length > 0) {
			config.setNumWorkers(3);  
			try {
				StormSubmitter.submitTopology(args[0], config, builder.createTopology());  
			} catch (Exception e) {
				e.printStackTrace();
			}
		} else {
			// 说明是idea里面本地运行
			config.setMaxTaskParallelism(20);  
			
			LocalCluster cluster = new LocalCluster();
			cluster.submitTopology("WordCountTopology", config, builder.createTopology());  
			
           	//idea本地运行，主线程休眠60再停止，测试strom单词计数demo效果
			Utils.sleep(60000);
			cluster.shutdown();
		}
	}
```



# 6 库存服务及缓存服务

## 6.1 在库存服务中实现缓存与数据库双写一致性保障方案







## 6.2 spring boot整合ehcache的搭建以支持服务本地堆缓存







