

# 1 zookeeper安装(集群)

**1 将提供的zookeeper-3.4.5.tar.gz拷贝到/usr/local目录下。**

**2 对zookeeper-3.4.5.tar.gz进行解压缩：**

**3 对zookeeper目录进行重命名：**

```shell
mv zookeeper-3.4.5 zk
```

**4 配置zookeeper相关的环境变量**

```
vim  /etc/profile
```

**PS:zookeeper安装的前提是已经安装好了JVM**

```shell
#profile加入zk相关环境变量，并设置到path中
export ZOOKEEPER_HOME=/usr/local/zk
export PATH=$PATH:$ZOOKEEPER_HOME/bin

```

**5 设置配置文件**

```shell
cd zk/conf
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg
```

修改配置文件

```shell
#zoo.cfg中修改对应data目录位置
dataDir=/usr/local/zk/data
#新增对应集群配置，放在最后面即可
server.0=eshop-cache01:2888:3888	
server.1=eshop-cache02:2888:3888
server.2=eshop-cache03:2888:3888
```

**6 建立输出目录**

```shell
cd zk
#在zk的根目录下建立data，就是对应zoo.cfg中的dataDir的位置
mkdir data
```

**7 在对应输出目录建立myid文件**

```shell
cd data
#设置myid文件的值为0即可
vi myid
0
```

**8 集群的配置**

配置3节点集群

在另外两个节点上按照上述步骤配置ZooKeeper，使用scp将zk和.bashrc拷贝到机器2和机器3上即可。唯一的区别是标识号分别设置为1和2。

**9 启动并检查是否安装成功**

分别在三台机器上执行：

```shell
#启动
zkServer.sh start
#检查ZooKeeper状态：，应该是一个leader，两个follower
zkServer.sh status
```


jps：检查三个节点是否都有QuromPeerMain进程

![image-20210904172804844](亿级流量.assets/image-20210904172804844.png)

**扩展-scp使用：**

scp是用来将本服务器上的文件复制到其他服务器上的命令

```shell
#scp 目录/文件(目录就前加-r) 账户@ip地址:对应目录
#回车后需要验证对应服务器密码
scp -r zk root@192.168.31.102:/usr/local
scp  zk.taz root@192.168.31.102:/usr/local
```

# 2 kafka安装(集群)

## 2.1 前言简介

scala，很多比如大数据领域里面的spark（计算引擎）就是用scala编写的，kafka也是基于scala开发的

Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。

Scala 运行在 Java 虚拟机上，并兼容现有的 Java 程序。

Scala 源代码被编译成 Java 字节码，所以它可以运行于 JVM 之上，并可以调用现有的 Java 类库。

## 2.2 scala安装

1 将课程提供的scala-2.11.4.tgz拷贝到/usr/local目录下。

2 对scala-2.11.4.tgz进行解压缩

3 对scala目录进行重命名

```shell
mv scala-2.11.4 scala
```

4 配置scala相关环境变量

```shell
vim /etc/profile
```

环境变量

```shell
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
```

5 查看scala是否安装成功：

```shell
scala -version
```

![image-20210905143338295](亿级流量.assets/image-20210905143338295.png)

6 按照上述步骤在其他机器上都安装好scala

## 2.3 kafka安装(集群)

1 将课程提供的kafka_2.9.2-0.8.1.tgz拷贝到/usr/local目录下

2 对kafka_2.9.2-0.8.1.tgz进行解压缩：

3 对kafka目录进行改名：

```shell
mv kafka_2.9.2-0.8.1 kafka
```

4 配置kafka

```
vim /usr/local/kafka/config/server.properties
```

配置文件

```shell
#依次增长的整数，0、1、2，集群中Broker的唯一id
broker.id=0
#集群配置
zookeeper.connect=192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181
```

5 安装slf4j

将课程提供的slf4j-1.7.6.zip解压
找到slf4j-nop-1.7.6.jar包
复制到kafka的libs目录下面

![image-20210905144508188](亿级流量.assets/image-20210905144508188.png)

6 解决kafka Unrecognized VM option 'UseCompressedOops'问题

```shell
#修改脚本文件
vim /usr/local/kafka/bin/kafka-run-class.sh 
```

脚本文件

```shell
if [ -z "$KAFKA_JVM_PERFORMANCE_OPTS" ]; then
  KAFKA_JVM_PERFORMANCE_OPTS="-server  -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true"
fi
#去掉-XX:+UseCompressedOops即可
```

去掉后

![image-20210905144643363](亿级流量.assets/image-20210905144643363.png)

7 集群配置

按照上述步骤在另外两台机器分别安装kafka。用scp把kafka拷贝到其他机器即可。
唯一区别的，就是server.properties中的broker.id，要设置为1和2

8 启动kafka集群

在三台机器上的kafka目录下，分别执行以下命令：

```shell
#启动
nohup bin/kafka-server-start.sh config/server.properties &
```

使用jps检查启动是否成功

![image-20210905144810149](亿级流量.assets/image-20210905144810149.png)

9 使用基本命令检查kafka是否搭建成功

```shell
#创建topic
bin/kafka-topics.sh --zookeeper 192.168.31.101:2181,192.168.31.102:2181,192.168.31.103:2181 --topic test --replication-factor 1 --partitions 1 --create

#创建生产者,这里会进入Kafka，输入相关信息，订阅对应topic的消费就能消费到
bin/kafka-console-producer.sh --broker-list 192.168.31.101:9092,192.168.31.102:9092,192.168.31.103:9092 --topic test
#创建消费者，这里也会进入kafka，就能展示对应topic生产的信息
bin/kafka-console-consumer.sh --zookeeper 192.168.31.101:2181,192.168.31.102:2181,192.168.31.103:2181 --topic test --from-beginning
```

生产

![image-20210905145439193](亿级流量.assets/image-20210905145439193.png)

消费

![image-20210905145522306](亿级流量.assets/image-20210905145522306.png)

# 3 亿级流量redis

参考笔记部分的redis内容

## 3.1 redis的LRU缓存清除算法

### 3.1.1、LRU算法概述

redis默认情况下就是使用LRU策略的，因为内存是有限的，但是如果你不断地往redis里面写入数据，那肯定是没法存放下所有的数据在内存的

所以**redis默认情况下，当内存中写入的数据很满之后，就会使用LRU算法清理掉部分内存中的数据，腾出一些空间来，然后让新的数据写入redis缓存中**

**LRU：Least Recently Used，最近最少使用算法**

### 3.1.2、缓存清理设置

**redis.conf**

**maxmemory**，设置redis用来存放数据的最大的内存大小，一旦超出这个内存大小之后，就会立即使用LRU算法清理掉部分数据

如果用LRU，那么就是将最近最少使用的数据从缓存中清除出去

对于64 bit的机器，如果maxmemory设置为0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止; 但是对于32 bit的机器，有一个隐式的闲置就是3GB

**maxmemory-policy**，可以设置内存达到最大闲置后，采取什么策略来处理

（1）noeviction: 如果内存使用达到了maxmemory，client还要继续写入数据，那么就直接报错给客户端
（2）allkeys-lru: 就是我们常说的LRU算法，移除掉最近最少使用的那些keys对应的数据
（3）volatile-lru: 也是采取LRU算法，但是仅仅针对那些设置了指定存活时间（TTL）的key才会清理掉
（4）allkeys-random: 随机选择一些key来删除掉
（5）volatile-random: 随机选择一些设置了TTL的key来删除掉
（6）volatile-ttl: 移除掉部分keys，选择那些TTL时间比较短的keys

### 3.1.3、缓存清理的流程

（1）客户端执行数据写入操作
（2）redis server接收到写入操作之后，检查maxmemory的限制，如果超过了限制，那么就根据对应的policy清理掉部分数据
（3）写入操作完成执行

### 3.1.4、redis的LRU近似算法

科普一个相对来说稍微高级一丢丢的知识点

**redis采取的是LRU近似算法，也就是对keys进行采样，然后在采样结果中进行数据清理**

redis 3.0开始，在LRU近似算法中引入了pool机制，表现可以跟真正的LRU算法相当，但是还是有所差距的，不过这样可以减少内存的消耗

**redis LRU算法，是采样之后再做LRU清理的，跟真正的、传统、全量的LRU算法是不太一样的**

**maxmemory-samples**，比如5，可以设置采样的大小，如果设置为10，那么效果会更好，不过也会耗费更多的CPU资源

### 3.1.5 LRU算法实现(java)

**待补充**

### 3.1.6 扩展

在redis里面，写入key-value对的时候，是可以设置TTL，存活时间，比如你设置了60s。那么一个key-value对，在60s之后就会自动被删除

redis的使用，各种数据结构，list，set，等等

allkeys-lru

这边拓展一下思路，对技术的研究，一旦将一些技术研究的比较透彻之后，就喜欢横向对比底层的一些原理

storm，科普一下

玩儿大数据的人搞得，领域，实时计算领域，storm

storm有很多的流分组的一些策略，按shuffle分组，global全局分组，direct直接分组，fields按字段值hash后分组

分组策略也很多，但是，真正公司里99%的场景下，使用的也就是shuffle和fields，两种策略

redis，给了这么多种乱七八糟的缓存清理的算法，其实真正常用的可能也就那么一两种，allkeys-lru是最常用的

# 4 ngnix+lua缓存架构

​	**用nginx+lua去开发，所以会选择用最流行的开源方案，就是用OpenResty**

​	**nginx+lua打包在一起，而且提供了包括redis客户端，mysql客户端，http客户端在内的大量的组件**

## 4.1 OpenResty安装

1 创建目录

```shell
mkdir -p /usr/servers  
cd /usr/servers/
```

2 安装必要依赖

```shell
yum install -y readline-devel pcre-devel openssl-devel gcc
```

3 下载包并解压

```shell
wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  
tar -xzvf ngx_openresty-1.7.7.2.tar.gz  
```

4 编译

```shell
#先进入对应目录
cd /usr/servers/ngx_openresty-1.7.7.2/
#先进入对应目录
cd bundle/LuaJIT-2.1-20150120/  
#编译
make clean && make && make install  
#连接
ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit
```

5 相关配置包

下载

```shell
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
#下载
wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  
#解压
tar -xvf 2.3.tar.gz  
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
#下载
wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  
#解压
tar -xvf v0.3.0.tar.gz  
```

编译

```shell
#进入目录
cd /usr/servers/ngx_openresty-1.7.7.2  
#编译
./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  
#编译
make && make install 
```

6 查看目录，启动nginx

![image-20210905202521243](亿级流量.assets/image-20210905202521243.png)

启动nginx:

```shell
 /usr/servers/nginx/sbin/nginx
```

**PS:若对应机器安装了gitlab会有nginx冲突，需要卸载gitlab，或者临时kill进程，或者改默认端口等操作，才能启动**

## 4.2 demo级应用

### 4.2.1 nginx+lua开发的demo

1 配置nginx.conf

```shell
vim /usr/servers/nginx/conf/nginx.conf
```

在http部分添加：

```shell
lua_package_path "/usr/servers/lualib/?.lua;;";  
lua_package_cpath "/usr/servers/lualib/?.so;;";  
include lua.conf;
```

2 创建lua.conf

/usr/servers/nginx/conf下，创建一个lua.conf

```shell
server {  
    listen       80;  
    server_name  _;  
    location /lua {  
    	default_type 'text/html';  
   		content_by_lua 'ngx.say("hello world")';  
	} 
}  
```

3 验证配置是否正确：

```shell
/usr/servers/nginx/sbin/nginx -t
```

4 重新nginx加载配置

```shell
/usr/servers/nginx/sbin/nginx -s reload  
```

访问http: http://192.168.31.187/lua

5 可选-独立出lua相关代码

```shell
#创建并打开
vim /usr/servers/nginx/conf/lua/hello.lua
#加入
ngx.say("hello world"); 
```

修改lua.conf

```shell
#相关代码改为引入文件
location /lua {  
    default_type 'text/html';  
    content_by_lua_file conf/lua/hello.lua; 
}
```

7 可选-查看异常日志

```
tail -f /usr/servers/nginx/logs/error.log
cat /usr/servers/nginx/logs/error.log
```



### 4.2.2 工程化的nginx+lua项目结构

项目工程结构

```
#这是目录层级
hello
    hello.conf     
    lua              
      hello.lua
    lualib            
      *.lua
      *.so
```

1 创建目录

```
mkdir /usr/hello
```

2 创建hello.conf文件

```shell
#打开并创建
vim /usr/hello/hello.conf
#加入
server {  
    listen       80;  
    server_name  _;  
  
    location /hello {  
        default_type 'text/html';  
        #lua_code_cache off;  
        content_by_lua_file /usr/hello/lua/hello.lua;  
    }  
} 

```

4 创建lua目录及对应代码文件

```shell
mkdir /usr/hello/lua
vim /usr/hello/lua/hello.lua
#加入
ngx.say("hello world 78910");
```

5 将对应包加入到hello工程目录中

```shell
cd /usr/hello/
#将之前装好的包移动到工程中
cd -r /usr/servers/lualib/ .
```

6 修改nginx配置文件，改为引用hello工程的相关配置

```shell
vim /usr/servers/nginx/conf/nginx.conf
#对应http中的配置改为
    lua_package_path "/usr/hello/lualib/?.lua;;";
    lua_package_cpath "/usr/hello/lualib/?.so;;";
    include /usr/hello/hello.conf;
```



![image-20210905204346847](亿级流量.assets/image-20210905204346847.png)

## 4.3 基于“分发层+应用层”双层nginx架构提升缓存命中率方案分析

**1、缓存命中率低**

**如果一般来说，你默认会部署多个nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的**

![缓存命中率低的原因](亿级流量.assets/缓存命中率低的原因.png)

**2、如何提升缓存命中率**

**分发层+应用层，双层nginx**

**分发层nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模**

**将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证说只会从redis中获取一次缓存数据，后面全都是走nginx本地缓存了**

后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器

看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能

![分发层+应用层双层nginx架构](亿级流量.assets/分发层+应用层双层nginx架构.png)



## 4.4 分发层nginx以及基于lua完成基于商品id的定向流量分发策略

### 4.4.1 实现逻辑

基于OpenResty在另外两台机器上都部署一下nginx+lua的开发环境

用101和102作为应用层nginx服务器，用103作为分发层nginx

在103，也就是分发层nginx中，编写lua脚本，完成基于商品id的流量分发策略

当然了，我们这里主要会简化策略，简化业务逻辑，实际上在你的公司中，你可以随意**根据自己的业务逻辑和场景，去制定自己的流量分发策略(提高缓存命中率)**

**1、获取请求参数，比如productId**

**2、对productId进行hash**

**3、hash值对应用服务器数量取模，获取到一个应用服务器**

**4、利用http发送请求到应用层nginx**

**5、获取响应后返回**

这个就是基于商品id的定向流量分发的策略，lua脚本来编写和实现

我们作为一个流量分发的nginx，会发送http请求到后端的应用nginx上面去，所以要先引入lua http lib包

### 4.4.2 具体实现

在对应目录加入lua脚本(相当于jar包依赖)

下载解压缩，复制到目录

https://www.zixuephp.net/uploads/file/20181017/1539739400144830.rar

```shell
cd /usr/hello/lualib/resty/  
```

编辑lua脚本

```shell
vim /usr/hello/lua/hello.lua
```

编辑

```lua
local uri_args = ngx.req.get_uri_args()
local productId = uri_args["productId"]

local hosts = {"192.168.31.101", "192.168.31.102"}
local hash = ngx.crc32_long(productId)
local index = (hash % 2) + 1
backend = "http://"..hosts[index]

local requestPath = uri_args["requestPath"]
requestPath = "/"..requestPath.."?productId="..productId

local http = require("resty.http")
local httpc = http.new()

local resp, err = httpc:request_uri(backend,{
  method = "GET",
  path = requestPath,
  keepalive = false
})

if not resp then
  ngx.say("request error: ", err)
  return
end

ngx.say(resp.body)

httpc:close() 
```

发送请求测试：

http://192.168.31.103/hello?requestPath=hello&productId=1

![image-20210906214634125](亿级流量.assets/image-20210906214634125.png)

**我们就可以看到，如果你请求的是固定的某一个商品，那么就一定会将流量打到固定的一个应用nginx上面去(提高缓存命中率)**

# 5 storm



# 6 库存服务及缓存服务

## 6.1 在库存服务中实现缓存与数据库双写一致性保障方案







## 6.2 spring boot整合ehcache的搭建以支持服务本地堆缓存







